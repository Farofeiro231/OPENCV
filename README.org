* Overview
This repository's goal is to hold my work for the Digital Image Processig course at UFRN. The programs here contained are of escalating complexity, starting with simple pixel manipulations in an image, going through video file manipulations until image frequency filtering.
* Pixel and region manipulation
Within the *pixel-manipulation* folder, there are a few simple programs. The first program (/pixels.cpp/) is a simple region manipulation one, where a strip of the image is painted in orange. The operation there performed is shown below. It works simply by opening an image file, determining a rectangular region to be written to and looping through its pixels to change their color one by one. I could certainly do this in a smarter and faster way, but the intent of the exercise was to make us used to working and manipulating pixels.

|----------------------+------------------------------|
| Original image       | Strip-modified image         |
|----------------------+------------------------------|
| [[./figuras/bolhas.png]] | [[./figuras/strip-modified.png]] |
|----------------------+------------------------------|

** Negative image

The /pixel-manip.cpp/ only opens the image passed as an argument and converts it to a grayscale image, hence I won't discuss it any further. The /regions.cpp/ program, however, is a bit more interesting; in it, the user passes an image image during the function call for the program to "negate" one portion of it; it works as follows: once the image is open, it is converted to a grayscale image (to simplify the procedure) and the user is asked for two (x, y) points, one for the upper-left corner of the negative rectangle to be created, and the other for the bottom-right corner. Once the two points are provided, every pixel within that zone has its value replace by the corresponding negative value; i.e.:

[[./figuras/negative-equation.png]]

 Below is the result of applying the negative effect to an area delimited by P1 = (50, 100) and P2 = (300, 500) on the Lenna image.

|----------------------+------------------------------|
| Original Lenna image | Modified Lenna image         |
|----------------------+------------------------------|
| [[./figuras/Lenna.png]]  | [[./figuras/negative-lenna.png]] |
|----------------------+------------------------------|

** Region-swapped image

The last program in this section is the one in the /swap-regions.cpp/ file. As its name suggests, this program takes an image and swaps its portions located in opposite quadrants (as in the for quadrants in a cartesian plan). The code mechanism is very simple; all of the work is done by the *swap_image* function: it takes the source image and the image (both are OPENCV Mat objects) that will receive the modified image resulting from the operation. Innside it, another Mat object of size of a single quadrant (i.e.: half the height and half the width of the original image) is created containing all zeros; then, one by one, the quadrants of the output image are replaced by the sum of this newly created all-zeros Mat with the opposing portion of the original image (the sum is necessary to create a new Mat object and to avoid modifying the original image in the swapping process). Running the program on the Lenna image yields the results shown bellow.

|----------------------+-----------------------------|
| Original Lenna image | Swapped Lenna image         |
|----------------------+-----------------------------|
| [[./figuras/Lenna.png]]  | [[./figuras/swapped-lenna.png]] |
|----------------------+-----------------------------|

* Labeling
This folder contains a simple program to detect and count the number of full and hollow bubbles (or contiguous regions distinct from the background) in a figure, as well as to highlight the found objects in the scene. To observe its results (which are divided in three different output images) I used the same bubbles image as the one in the previous section. Here's how the program works:

1) The user executes the program and passes an image file path as its argument;
2) The image is either open correctly - and the algorithm moves to the next step - or not, in which case the execution ends;
3) The image is then converted to a copy of itself in grayscale and displayed on screen, unaltered.
4) The grayscale copy of the image is then analyzed to have its edge objects removed (the objects touching one of the figure's four edges) - this is required since we cannot guarantee that edge objects are hollow of full, and we'll work on this later on -; therefore, all of the edge pixels are verified to look for white ones; if a white pixel is found, the OPENCV floodFill method is used to turn that whole bubble into a black one (making it disappear in the dark background);
5) After eliminating the edge bubbles, the hollow ones are now detected and have their inner edges drawn in gray. This is done by using OPENCV's findCountours method, which allows not only for contour detection, but also for the storage of the contours' hierarchical information in an array named << hierarchy >>; from this array, then, I check which contours have children (inner contours, what indicates a hollow bubble) and increment the number of hollows found;
6) Then, the bubbles are counted through the search of white pixels and the usage, once more, of the floodFill method: each found object is then counted (starting on zero) and its count number is used to replace its original gray intensity (here a problem arises: if the number of bubbles is higher than 255 - the uchar limit -, then I'd have to use an integer to count them, and another strategy to change their color);
7) To conclude, to image with the counted objects undergoes an histogram equalization (using opencv's equalizeHist method), which makes the bubble more visible (mainly the first ones detected, since their colors were replaced by very low gray intensities).



|----------------------+-------------------------------------|
| Original image       | Edgless image and contoured bubbles |
|----------------------+-------------------------------------|
| [[./figuras/bolhas.png]] | [[./figuras/edgeless-contoured.png]]    |
|----------------------+-------------------------------------|

|-----------------------+-----------------------------------|
| Counted bubbled       | Counted and Highlighted bubbles   |
|-----------------------+-----------------------------------|
| [[./figuras/counted.png]] | [[./figuras/counted-highlighted.png]] |
|-----------------------+-----------------------------------|

* Histogram manipulation
Entering now the realm of more complex manipulations and starting to work with colored images, this portion of the activity handles histogram manipulations. The first program here performs a simple histogram equalization on an image, and the second one uses the differences on frames' histograms to detect motion in a video captured scene.
** Equalization
The equalization program (/equalize.cpp/ inside the manip-histogram folder) performs an histogram equalization on the frames captured by the default video device on the computer, and display both the original video and its equalized version on-screen. The equalized version has, as predicted, a slight delay in comparison to the original video (as it has to perform a few operations on each frame before displaying them).

The first procedure in the program is to initialize the VideoCapture object (named *cap*) on line 93 (after declaring all the other variables). If the video capture device cannot be open for whatever reason, the execution ends; however, if the device is available, the object has, then, connfigured its captured frames' height and width. On lines /110/ to /114/ are initialized the objects that will hold mini-plots of the current frame's histogram for each image (the red, blue and green histograms will, each, be displayed in a small rectangle on the top left corner of the captured image). Then, the program goes on the loop responsible for equalizing the incoming frame's histogram and showing both the original frame and the equalized version on screen.

Once inside the loop, each frame has its histogram - for each color plane - normalized and histored in the *histR*, *histG*, and *histB* variables. This is done at line 121:

#+BEGIN_SRC CPP
  equalizeHistogram(image, planes, histR, histG, histB, nbins, histrange);
#+END_SRC

The above function operates, then, splits the original image into three new images, each one containing only the information corresponding to a given color's intensity (for the red, the blue and the green colors).

#+BEGIN_SRC CPP
  cv::split(src, planes);  // src is the orignal image in the current frame and planes is an array of opencv matrices
#+END_SRC

These single-color images are then used to calculate the histogram of each color component in the original image. However, the plot will reflect the histogram trend in a simplified manner, as having one bin for each intensity (rangin from 0 to 255) would create a very large plot over the current frame; hence, I chose to use 64 bins on the histogram plot to make it smaller. Therefore, the intensity jump from one bin to its neighboor is of 4 values (e.g.: red intensities from 0 to 3 will land on the same bin, but an intensity of 4 will be in the next bin).

From the information of each color component's histogram, is calculated the accumulated histogram for each one of them, which is in turn used to equalized each color plane individually. After performing the equalization of each plane, the three of them are merged back into the original image. Here, as the original image has already been displayed on screen, there's no problem in overwriting it.

A few more steps are necessary to plot the histogram graphs on top of the equalized image. First, it is necessary to normalize each histogram to fit the size of the small plotting windows where it will be plotted. Here's how it's done:

#+begin_src cpp
    cv::normalize(histR, histR, 0, histImgR.rows, cv::NORM_MINMAX, -1, cv::Mat());
    cv::normalize(histG, histG, 0, histImgG.rows, cv::NORM_MINMAX, -1, cv::Mat());
    cv::normalize(histB, histB, 0, histImgB.rows, cv::NORM_MINMAX, -1, cv::Mat());
#+end_src

Then, the small plotting areas are set to black (lines 127 to 129) and the histograms are plotted on top of each one; there's a catch, though: as the origin of the coordinate system is on the top-left corner of the image, each bin needs to be drawn as a line going from the botttom up, as shown bellow) - this could be turned into a function for less code duplication.

#+begin_src cpp
  for(int i = 0; i < nbins; i++){
    cv::line(histImgR,
             cv::Point(i, histh),
             cv::Point(i, histh-cvRound(histR.at<float>(i))),
             cv::Scalar(0, 0, 255), 1, 8, 0);
    cv::line(histImgG,
             cv::Point(i, histh),
             cv::Point(i, histh-cvRound(histG.at<float>(i))),
             cv::Scalar(0, 255, 0), 1, 8, 0);
    cv::line(histImgB,
             cv::Point(i, histh),
             cv::Point(i, histh-cvRound(histB.at<float>(i))),
             cv::Scalar(255, 0, 0), 1, 8, 0);
   }
#+end_src

After that, each histogram's plot is copied on top of the now-equalized image and the result is shon on screen. The user can stop the program by pressing the ESC key.

Here's an example of the outputs of the program for a given frame. The equalization effect is most useful when the environment in which the pictures are taken is either too bright or too dark.

|-----------------------------------------------------------------|
| Original image (left) / Equalized image with plotted histograms |
|-----------------------------------------------------------------|
| [[./figuras/equalized_book.png]]                                    |
|-----------------------------------------------------------------|

** Histogram-based motion detector

This program is based on the previous one (the equalization program), hence, most of it works just as the the other. The difference here, however, is this: two frames are kept at a time, and their histograms are compared to verify if there are any important discrepancies; if that's the case, a red dot appears on the screen's bottom-right corner to indicate that motion has been detected.

#+begin_src cpp
  // Two images are kept at once.
  cap >> image_t;
  cap >> image_t_plus;
#+end_src

To detect any motion, the bellow function (whose name is very suggestive) is used.

#+begin_src cpp
  detect_motion(image_t, image_t_plus, planes, nbins, histrange);
#+end_src

It takes the image at instant /t/ and the image at /t + 1/ and compared their histograms. Inside the function, both frames are split into three images containing, each, a single color component (just as was done for the equalization function). Then, another function called /histogram_diff/ is used to calculate the maximum difference for each color component between the two frames.

#+begin_src cpp
  hist_max[0] = histogram_diff(histB, histB_plus);
  hist_max[1] = histogram_diff(histG, histG_plus);
  hist_max[2] = histogram_diff(histR, histR_plus);
#+end_src

These difference values are then compared with movement thresholds (set up beforehand) to check for any movement; if any one of them is higher than the given threshold, the red dot is printed on screen to indicate the detection. The bellow image shows an instant where I quickly moved the book to prompt the motion detection.


|-------------------------------|
| Motion detected               |
|-------------------------------|
| [[./figuras/motion-detected.png]] |
|-------------------------------|

* Space filtering
In this section I start dealing with masks applied to images in the space domain (without using any kind of transformation such as Fourier's transform). The first portion of this exercise comprises a simple task: to add an LoG (Laplaciann of Guassian) filter to the program created by my professor. The file in question is the /spacefilter.cpp/ inside the *space-filtering* folder.

** Laplacian-of-Gaussian filterinng

The readers can try for themselves all of the filters in the program. To swap between filters, it suffices to hit the corresponding key after launching the program (e.g.: 'b' for the boost filter, or 'l' for the laplacian filter). The program works in a very simple manner: each filter's mask is pre-defined inside the main fuction as a float-array. Then, the program goes into a loop that waits for a key press and changnes the mask applied to the image accordingly. At the beginning of each loop, the original image is filtered through the application of the selected mask via the usage of opencv's filter2D method: it applies the mask as a kernel on the original image. The masks definition is shown bellow.

#+begin_src cpp
  float media[] = {0.1111, 0.1111, 0.1111, 0.1111, 0.1111,
                   0.1111, 0.1111, 0.1111, 0.1111};
  float gauss[] = {0.0625, 0.125,  0.0625, 0.125, 0.25,
                   0.125,  0.0625, 0.125,  0.0625};
  float vertical[] = {-1, 0, 1, -2, 0, 2, -1, 0, 1};
  float horizontal[] = {-1, -2, -1, 0, 0, 0, 1, 2, 1};
  float laplacian[] = {0, -1, 0,
                       -1, 4, -1,
                       0, -1, 0};
  float boost[] = {0, -1, 0,
                   -1, 5.2, -1,
                   0, -1, 0};
  float LoG[] = {0, 0, 1, 0, 0,
                 0, 1, 2, 1, 0,
                 1, 2, -16, 2, 1,
                 0, 1, 2, 1, 0,
                 0, 0, 1, 0, 0};
#+end_src

To implement the Laplacian-of-Gaussian filter, I simply used this widely known mask and added another option for the the case selection inside the main loop.

#+begin_src cpp
  case 'p':
  // The performance of the LoG is further improved by filtering it
  // with the average mask afterwards.
  mask = cv::Mat(5, 5, CV_32F, LoG);
  printmask(mask);
  break;
#+end_src

The images bellow show a comparison between the Laplacian and the LoG filters applied to the book image. As we can see, the LoG filter sharpens the contours of the image at the cost of being more sensitive to noise.


|-------------------------+--------------------|
| Guassian-filtered image | LoG-filtered image |
|-------------------------+--------------------|
| [[./figuras/laplacian.png]] | [[./figuras/log.png]]  |
|-------------------------+--------------------|

** Image tilt-shifting

This program (/addweighted.cpp/) performs a transformation on the image similar to the tilt-shift technique used by photographers to blurry selected regions of an image. This effect is also used to create miniature-like versions of pictures. Normally, this effect is produced with the help of special lenses; nevertheless, it can be easily reproduced by the use of some digital image processing techniques. The output image after the application of the transformation and an image with a region under focus, and the surrounding areas blurred.

Now, let me explain how the program works. Its basic structure was provided to me by the course's professor, but it was properly modified and expanded to account for the desired transformation. The original program performed a superposition of an image with its transformed version after an average filtering. It had two control sliders: one to control the transparency (alpha) of the superposed image (the blurred one); the other to control the height until which the blurred image would be seen on top of the original one (this is called scanline). In the same window the user could see the output image and the controlling sliders.

The modified program, built on top of that one, has kept the origiannl sliders simply for the user to be able to see the old effect applied to the image; however, it had a few features added to produce the tilt-shift effect. The first noticeable change is the addition of a second visualization window (as shown in  the figure below): it allows the user to control, via three sliders (height, width and intensity) the positioning of the tilt-shift mask's center, as well as its width and decay intensity around the borders; the user can see, in real time, the mask produced by changing each slider. To exit the program, the user simply presses the ESC key.

|----------------------------+---------------------------------------|
| Output image window        | Mask control and visualization window |
|----------------------------+---------------------------------------|
| [[./figuras/output-image.png]] | [[./figuras/mask.png]]                    |
|----------------------------+---------------------------------------|

Now, let's see how the algorithm works. At first, two image objects are created: *image1* and *image2*. The former is created from the opening of the image file defined in the code, and the latter is created after applying an average filter onto the original image.

#+begin_src cpp
  average_filter(image1, image2);
#+end_src

Then, from lines 164 to 196, are created the sliders used in both windows. Here is introduced the important part: each slider has a function that is called whenever the slider's value is modified; for the mask's window sliders, the function is the same for all three sliders: the *mask_control* function.

The *mask_control* function does exactly what its name suggests, as it controls the mask behavior and updates the mask (maybe it would be better named as *mask_update*, or even *outcome_update*, as that what it does ultimately). Inside this function, the first action is perform by another function, the *modify_mask* function; that's the one, precisely, that modifies and updates the mask.

#+begin_src cpp
  void mask_control(int, void*)
  {
    modify_mask(mask);
    ...
#+end_src

The *modify_mask* function, in its turn, starts by turning the mask into a black image upon which the final mask will be drawn. It then calculates the increment step, based on the width of the transition regions (one for the pixels above the focus height, and other for those below it) or the remaining pixels until the corresponding end of the image (whichever is lower, as this is needed to prevent odd behaviors when the the combination of height and width makes the end of the transition region outside of the image's limits).

#+begin_src cpp
  // Determining the upper and lower steps
  if ((focus_height - focus_width / 2.0) >=0)
    upper_step =  (1.0 / (focus_width / 2.0)) * focus_intensity;
   else
     upper_step = (1.0 / focus_height) * focus_intensity;

  if ((focus_height + focus_width / 2.0) <= mask.rows)
    lower_step = (1.0 / (focus_width / 2.0)) * focus_intensity;
   else
     lower_step = (1.0 / (mask.rows - focus_height)) * focus_intensity;
#+end_src

For each pixel within the mask's transition zone, hence, the grayscale intensity of the pixel is incresed, on the upper half, by the upper step, and decreased by the lower step in the lower half transition zone. To make sure the 255 limit for unsigned chars (the data type used for grayscale images - the type of our mask) is not exceeded, a test is conducted on each line of the mask's transition region to ensure each pixel's new value will be, at most, 255.

After the creation of the mask according to the slider settings of the Mask window, there is the creation of what I called the /blended/ image; that is, the sum of the original image multiplied (element-wise) by the mask, with the blurred image multiplied (element-wise as well) by 1 - mask. This is performed in lines 127 to 133 of the code (as shown bellow).

#+begin_src cpp
  ...
  imageBottom = image1.mul(mask * (1.0 / 255));

  // When using the cv::Mat::ones method only the first channel is initialized to 1,
  // therefore I need to initialize the other two myself. I used an ordinary initialization instead.

  imageTop = image2.mul(cv::Mat(mask.rows, mask.cols, CV_8UC3, CV_RGB(1, 1, 1)) - mask * (1.0 / 255));
  blended = imageTop + imageBottom;
  cv::imshow("addweighted", blended);
  }
#+end_src

** Video tilt-shifting

The last program of this section is the result of a slight modification in the previous one. The goal is no longer to apply the tilt-shift effect to a single image, but rather to every frame of a video file. The program here described is contained in the *space-filtering/src/video_manip.cpp* file. I'll then focus on the differences between both files and the new functionnalities added to this one.

The first thing to notice is the creation of a new function called *modify_video* that takes the original VideoCapture object and uses its frames to create the modified video. The output of this function is a video file called *modified.avi* where the new video will be stored; it has the original video's frames transformed through the application of the tilt-shift effect on them. A #+begin_src cpp cv::VideoWriter #+end_src object is created to write to the new video file, and to each frame is applied the *modify_frame* function. In the end, the VideoWriter object is released.

#+begin_src cpp
  void modify_video(cv::VideoCapture &original_video)
  {
    int frame_width = original_video.get(cv::CAP_PROP_FRAME_WIDTH);
    int frame_height = original_video.get(cv::CAP_PROP_FRAME_HEIGHT);
    cv::Mat current_frame;
    cv::Mat modified_frame;

    cv::VideoWriter modified_video("modified.avi", cv::VideoWriter::fourcc('M', 'J', 'P', 'G'), 30,
                                   cv::Size(frame_width, frame_height));

    // Here we'll loop through the video frames until the video file has been read in its entirety.
    // Each frame will be modified according to the selected mask.
  
    while (1) {

      original_video >> current_frame;

      if (current_frame.empty()) {
        break;
      }

      modified_frame = modify_frame(current_frame);
      modified_video.write(modified_frame);

      // Display the current frame
      imshow("Frame", modified_frame);
    }

    // Releasing the memory used by the VideoWriter object
    modified_video.release();
  }
#+end_src

The *modify_frame* function tackles the work of applying the tilt-shift effect to the frame in question. It receives a single argument (the current frame's object), calculates the reverse_mask (which will be applied to the blurried version of the frame) and applies each mask to their due versions of the frame (the normal to the original frame, and the reverse mask to the blurried one). Finally, the output frame is generated by the addition of both version of the frame (cf. bellow listing).

#+begin_src cpp
  cv::Mat& modify_frame(cv::Mat &original_frame)
  {
    static cv::Mat averaged_frame;
    static cv::Mat reverse_mask;
  
    reverse_mask = cv::Mat(mask.rows, mask.cols, CV_8UC3, cv::Scalar(255, 255, 255)) - mask;
    average_filter(original_frame, averaged_frame);
    imageBottom = original_frame.mul(mask * (1.0 / 255));

    // When using the cv::Mat::ones method only the first channel is initialized to 1,
    // therefore I need to initialize the other two myself. I used an ordinary initialization instead.


    imageTop = averaged_frame.mul(reverse_mask * (1.0 / 255));
    blended = imageTop + imageBottom;
    return blended;
  }
#+end_src

To conclude, when the user has obtained the desired mask (which can be seen, both it and its reverse counterpart, on screen - a new window has been added to displaye the reverse mask), it suffices to hit the SPACEBAR key to begin the modified video's generation process. Once it's done the program will free all allocated video objects and close itself.


|---------------------------+-----------------------------------|
| Mask visualization window | Reverse mask visualization window |
|---------------------------+-----------------------------------|
| [[./figuras/video-mask.png]]  | [[./figuras/reverse-mask.png]]        |
|---------------------------+-----------------------------------|

* Frequency filtering
In this section I'll explain the programs contained in the freq-filtering folder, which are all based on transformations using an image's Fourier transform. The first program comprises an homomorphic filter destined to reduced the effect of bad lighting on captured scenes. The second one is used to create a "dotted" (from the dotillistic painting style) image from its original version.
** Homomorphic filter
This program is contained in the /dft.cpp/ file, and it was built by performing a simple modification to the original file provided by my professor (most of the original files used here are available at [[https://agostinhobritojr.github.io/tutorial/pdi/][Agostinho's Github.io]]). The original file already had in it the implementation for all of the other filters (gaussian, median etc.) and frequency-based image manipulations. The idea behind this program was to add a functionnality to apply an homomorphic filter on each frame of the captured image (the default video - 0 - capture device is used here).

The first thing modified was the user interaction menu, to which was added the << /h/ >> option, allowing the user to apply the homomorphic filter to the incoming frames.

#+begin_src cpp
  void menu() {
    std::cout << "e : habilita/desabilita interferencia\n"
      "m : habilita/desabilita o filtro mediano\n"
      "g : habilita/desabilita o filtro gaussiano\n"
      "h : habilita/desabilita o filtro homomórfico\n"
      "p : realiza uma amostra das imagens\n"
      "s : habilita/desabilita subtração de fundo\n"
      "b : realiza uma amostra do fundo da cena\n"
      "n : processa o negativo\n";
  }
#+end_src

Another boolean variable called *homomorphic* was created to be toggled when the user presses the << /h/ >> key. This is done on line 80 of the file.

#+begin_src cpp
  // habilita o filtro homomorfico
  bool homomorphic = false;
#+end_src

To perform the homomorphic filtering, another filter matrix was created (*filter_homo*) sized with the optimal dimensions calculated at lines 124 and 125.

#+begin_src cpp
  // identifica os tamanhos otimos para
  // calculo do FFT
  dft_M = cv::getOptimalDFTSize(image.rows);
  dft_N = cv::getOptimalDFTSize(image.cols);
#+end_src

This filter's creation is based on the following version of the Gaussian filter; it's meant to atenuate lower frequencies (thus to remove disparities in illuminance) and keep higher frequencies untouched (ideally).

[[./figuras/homo-equation.png]]

In the above equation, four parameters are used to control the behavior of the filter: \gamma_{L}, \gamma_{H}, c and D_{0}. After a few attempts to find their best combination, I've settled for the following values. The radius (D_{0}) controls the size of the atenuation region, the gammas control the size of the transition region, and the *c* controls the intensity of the transition.

#+begin_src cpp
  #define RADIUS 30
  #define HOMO_C 2
  #define GAMMA_L 5
  #define GAMMA_H 60
#+end_src

After creating the filter's matrix, another auxilliary matrix was created to hold a single plane of the filter (since the filter has two channels: one for its real and another one for its complex component). This auxilliary matrix is then filled with values according to the homomorphic equation, considering the origin of the image spectrum to be at the center of the image (due to some manipulations performed on the image beforehand). This procedure is performed in the lines below (lines 177 to 189 of the original file).

#+begin_src cpp
  // Repetição do procedimento acima para a criação da matriz homomófica.
  tmp_homo = cv::Mat(dft_M, dft_N, CV_32F);
  float d_u_v_squared = 0;

  // Criação do filtro homomórfico usando os parâmetros c, gamma_L, gamma_H e RADIUS
  for (int i = 0; i < dft_M; i++) {
    for (int j = 0; j < dft_N; j++) {
      d_u_v_squared = (i - dft_M / 2.0) * (i - dft_M / 2.0) + (j - dft_N / 2.0) * (j - dft_N / 2.0);
      // tmp_homo.at<float>(i, j) = ((GAMMA_H - GAMMA_L) * (1 - exp(-HOMO_C * (d_u_v_squared / (RADIUS * RADIUS)))) + GAMMA_L) / ((GAMMA_H - GAMMA_L) + GAMMA_L);
      tmp_homo.at<float>(i, j) = ((GAMMA_H - GAMMA_L) * (1 - exp(-HOMO_C * (d_u_v_squared / (RADIUS * RADIUS)))) + GAMMA_L);
    }
   }
  cv::normalize(tmp_homo, tmp_homo, 0, 1, cv::NORM_MINMAX);
#+end_src

The *d_u_v_squared* hold the squared distance of the current point in the filter's matrix to its center. It's important to note that all values applied to the filter elements are normalized between 0 and 1.

Finally, if the homomorphic filtering has been toggled by the user, the homomorphic filter will be used instead of the ideal filter.

#+begin_src cpp
  // aplica o filtro de frequencia: a variável filter é um filtro
  // ideal com componentes reais e imaginárias iguais.
  if (!homomorphic)
    cv::mulSpectrums(complexImage, filter, complexImage, 0);
   else
     cv::mulSpectrums(complexImage, filter_homo, complexImage, 0);
#+end_src

At this point, the image has already been filtered and we're ready to perform the inverse transformation in order to obtain its space-domain filtered version. The results of the filtering applied to an image of a book with strong direct lighting is seen bellow.

|-----------------------------+-----------------------------|
| Original imagem             | Homomorphic-filtered image  |
|-----------------------------+-----------------------------|
| [[./figuras/homo_original.png]] | [[./figuras/homo-filtered.png]] |
|-----------------------------+-----------------------------|

** Dotillism art
One usage of space-domain image manipulations is the creation of images simulating the painting style called dotillism, and the results are very good considering the little effort put into implementing such solutions. However, one can improve (or modify) the output image by using some image manipulations taking place in the frequency domain. One such technique, employed here in the //freq-filtering/src/cannypoints.cpp/ file is to apply an edge-detection algorithm on the original image and then to use the pixels composing the edges to insert new, smaller circle on top of the first dotted image.

The barebones of the program, including the algorithm to create the "dotted" image was provided by the professor, and it was asked for us to implement an improved version of the dotillism effect by making use of the Canny edges. The function that does all the work is shown bellow.

#+begin_src cpp
  cv::Mat edge_dotillism(cv::Mat& image){
    vector<int> yrange;
    vector<int> xrange;

    Mat frame, points;
    static Mat new_image, border_image;

    int width, height, gray;
    int x, y;

    width=image.size().width;
    height=image.size().height;

    xrange.resize(height/STEP);
    yrange.resize(width/STEP);

    iota(xrange.begin(), xrange.end(), 0);
    iota(yrange.begin(), yrange.end(), 0);

    Canny(image, border, top_slider, 3 * top_slider);
    cv::normalize(border, border, 0, 1, cv::NORM_MINMAX, -1, cv::Mat());
    border_image = image.mul(border);
  
    for(uint i=0; i<xrange.size(); i++){
      xrange[i]= xrange[i]*STEP+STEP/2;
    }

    for(uint i=0; i<yrange.size(); i++){
      yrange[i]= yrange[i]*STEP+STEP/2;
    }

    points = Mat(height, width, CV_8U, Scalar(255));

    random_shuffle(xrange.begin(), xrange.end());

    for(auto i : xrange) {
      random_shuffle(yrange.begin(), yrange.end());
      for(auto j : yrange) {
        x = i+rand()%(2*EDGE_JITTER)-EDGE_JITTER+1;
        y = j+rand()%(2*EDGE_JITTER)-EDGE_JITTER+1;
        gray = image.at<uchar>(x,y);
        circle(points,
               cv::Point(y,x),
               RAIO,
               CV_RGB(gray,gray,gray),
               -1,
               LINE_AA);
      }
    }

    for (int i = 0; i < border_image.rows; i++) {
      for (int j = 0; j < border_image.cols; j++) {
        if (border_image.at<uchar>(i, j) != 0) {
          gray = border_image.at<uchar>(i, j);
          circle(points, cv::Point(j, i), EDGE_RAIO, CV_RGB(gray, gray, gray), -1, LINE_AA);
        }
      }
    }

    imwrite("pontos.jpg", points);
    return points;
  }
#+end_src

At first, the user runs the program passing an image path as its sole argument; this is the image taken as input by the *edge_dotillism* function. Once the function is called, it uses a few parameters to generate coordinates that's will be used to draw the circles using the grayscale intensities of the original image. From the parameters below, the *STEP* parameter indicates the length of the side of the adjacent squares from which each point will be taken to retrieve the circle intensity that will replace that region. The size of the circle is determined by the *RAIO* macro. The *EDGE_JITTER* one indicates how randomly will the points be selected, and the *EDGE_RAIO* determines the size of the second circles that are drawn based on the edge's pixels positions (from the Canny algorithm).

#+begin_src cpp
  #define STEP 5
  #define EDGE_JITTER 1
  #define RAIO 3
  #define EDGE_RAIO 2
#+end_src

Then, from lines 41 to 79 the dotted effect is generated using the randomly selected points of the original image as the color intensity generators for the new image. Before we move on to the differential of the code, however, it's important to talk about the generated image from the output of the Canny processing. In the lines bellow the Canny processing is applied on the original image and the detected borders are stored on the *border* matrix (which is in turn normalized to hold either zeros or ones). Then, a new image is created containing only the pixels on the detected borders, but with their original intensity (ranging from 0 to 255). This is the image that will be used to generate the second round of circles drew on the already dotted image.

#+begin_src cpp
  Canny(image, border, top_slider, 3 * top_slider);
  cv::normalize(border, border, 0, 1, cv::NORM_MINMAX, -1, cv::Mat());
  border_image = image.mul(border);
#+end_src

Finally, the central part, the one that generates the additional touch, comes at line 81. There, every pixel of *border_image* matrix is checked for values different from zero; if one such pixel is found, its intensity is used to draw a circle on its location whose radius is equal to the *EDGE_RAIO* parameter. After that, the final art is ready and is displayed on screen. Bellow is the result of this second processing.

|---------------------------------+--------------------------------|
| Dotted image (simple dottilism) | Modified dotted image          |
|---------------------------------+--------------------------------|
| [[./figuras/dot-art.jpg]]           | [[./figuras/modified-dot-art.jpg]] |
|---------------------------------+--------------------------------|


* K-Means
This section deals with a different type of problem: vector quantization of images. Here I've worked with the Kmeans clustering algorithm provided by the OPENcv library, and I've used the code at //kmeans/src/kmeans.cpp/ (provided by the professor) as the foundation for the observations here presented. This was a simple matter of observing the effect of a Kmeans clustering with random initialization of the cluster centroids. The parameters of interest here are shown bellow inside the TermCriteria argument of the Kmeans function, as well as in the KMEANS_RANDOM_CENTERS argument.

#+begin_src cpp
  kmeans(samples,
         nClusters,
         rotulos,
         TermCriteria(TermCriteria::COUNT | TermCriteria::EPS, 10000, 0.0001),
         nRodadas,
         KMEANS_RANDOM_CENTERS,
         centros);
#+end_src

Above, we can see that two arguments inside the TermCriteria constructor are telling the Kmeans to stop trying to find best clusters as soon as the desired accuracy (/epsilon/) is reached (in this case, 0.0001), or the desired amount of attempts has been reached (10000). However, to see the effect of random centroids initialization, I used the KMEANS_RANDOM_CENTERS argument, which initializes the centroids randomly, in opposition to the other possible argument, KMEANS_PP_CENTERS, which makes the Kmeans to look for optimal initial centroids. To further ensure that the result will not converge to the same centroids each time, I've limited the number of clustering rounds to use by setting *nRodadas = 1*; this way we're sure to see the different clusters selected everytime. Finally, so we can understand the output of the program, it suffices to know that each cluster (for each color channel, as they're treated separately) is given one color intensity (and their quantity is defined by the *nClusters* variable), therefore, the output will have much less color than the original image.

Below is the output of the program after five indepent runs on the same image.

|-----------------------+------------------------+------------------------|
| Original imagem       | First run              | Second run             |
|-----------------------+------------------------+------------------------|
| [[./figuras/lobster.jpg]] | [[./figuras/lobster1.jpg]] | [[./figuras/lobster2.jpg]] |
|-----------------------+------------------------+------------------------|
